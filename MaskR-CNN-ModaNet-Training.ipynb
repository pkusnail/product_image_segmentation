{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "################################ GLOBAL VARIABLES ############################\n",
    "\n",
    "########## DATA ##########\n",
    "MODANET_ANNO = \"modanet/annotations/modanet2018_instances_train.json\"\n",
    "MODANET_VAL =  \"modanet/annotations/modanet2018_instances_val.json\"\n",
    "PAPERDOLL_META_DB = \"data/paperdoll/chictopia.sqlite3\"\n",
    "PAPERDOLL_IMG_DB =  \"data/paperdoll/photos.lmdb\"\n",
    "\n",
    "\n",
    "\n",
    "########## MODEL ##########\n",
    "JOB_NAME = \"MODANET\"\n",
    "MODEL_TYPE = 'resnet50'\n",
    "NUM_CATS = 13\n",
    "IMAGE_SIZE = 512 #INPUT IMAGE SIZE : IMAGE_SIZE * IMAGE_SIZE\n",
    "\n",
    "GPU_NUM = 1 #1# 2# 4# #8\n",
    "IMAGES_PER_GPU_PER_IT = 4 #4 \n",
    "\n",
    "STEPS_PER_EPOCH_NUM = 500 #40000\n",
    "VALIDATION_STEPS_NUM = 100\n",
    "\n",
    "LR = 1e-4\n",
    "#EPOCHS = [2, 4, 8, 16, 32, 64]\n",
    "EPOCHS = [2, 4, 8, 9, 10, 11]\n",
    "\n",
    "N_FOLDS = 5\n",
    "SELECT_FOLD_IDX = 3\n",
    "\n",
    "## pretrain data\n",
    "FGVC6_WEIGHT_FIEL =  CURR_PATH + \"/weight_files/mask_rcnn_FGVC6_0008.h5\" \n",
    "LOG_DIR =  CURR_PATH + \"/weight_files/logs\"\n",
    "\n",
    "###########################################################################\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import datetime\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage.draw\n",
    "import skimage.io as imio\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from imgaug import augmenters as iaa\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import pylab\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "import lmdb\n",
    "import sqlite3\n",
    "\n",
    "sys.path.append( \"./mask_rcnn_lib\")  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import model as modellib, utils\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools import mask as maskUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################################################\n",
    "#  Configurations\n",
    "############################################################\n",
    "#Photo data in LMDB\n",
    "\n",
    "class PhotoData(object):\n",
    "    def __init__(self, path):\n",
    "        self.env = lmdb.open(\n",
    "            path, map_size=2**36, readonly=True, lock=False\n",
    "        )\n",
    "        \n",
    "    def __iter__(self):\n",
    "        with self.env.begin() as t:\n",
    "            with t.cursor() as c:\n",
    "                for key, value in c:\n",
    "                    yield key, value\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        key = str(index).encode('ascii')\n",
    "        with self.env.begin() as t:\n",
    "            data = t.get(key) # binary image data\n",
    "        if not data:\n",
    "            return None\n",
    "        \n",
    "        with io.BytesIO(data) as f:\n",
    "                image = Image.open(f)\n",
    "                try:\n",
    "                    image = image.convert(\"RGB\")\n",
    "                    image.load()\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "        return np.array(image)\n",
    "    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.env.stat()['entries']\n",
    "\n",
    "\n",
    "class ModaNetConfig(Config):\n",
    "    def __init__(self, class_num=NUM_CATS, config_name=JOB_NAME):\n",
    "        self.NUM_CLASSES = class_num + 1 # +1 for the background class\n",
    "        self.NAME = config_name\n",
    "        super().__init__()\n",
    "      \n",
    "    BACKBONE = MODEL_TYPE\n",
    "    \n",
    "    \"\"\"     \n",
    "    GPU_COUNT = GPU_NUM\n",
    "    IMAGES_PER_GPU = IMAGES_PER_GPU_PER_IT \n",
    "\n",
    "    \n",
    "    IMAGE_MIN_DIM = IMAGE_SIZE\n",
    "    IMAGE_MAX_DIM = IMAGE_SIZE    \n",
    "\n",
    "    RPN_ANCHOR_SCALES = (16, 32, 64, 128, 256)\n",
    "    \n",
    "    STEPS_PER_EPOCH = STEPS_PER_EPOCH_NUM\n",
    "\n",
    "    # Skip detections with < 90% confidence\n",
    "    #DETECTION_MIN_CONFIDENCE = 0.9\n",
    "    \"\"\"    \n",
    "\n",
    "\n",
    "############################################################\n",
    "#  Dataset\n",
    "############################################################\n",
    "\n",
    "class ModaNetDataset(utils.Dataset):\n",
    "    def __init__(self, photo_meta_db, coco, label_names, cat_ids):\n",
    "        \n",
    "        super().__init__(self)\n",
    "        self.coco = coco\n",
    "        self.label_names = label_names\n",
    "        self.catIds = cat_ids\n",
    "        self.photo_meta_db = photo_meta_db\n",
    "        self.imgid2idx = {}\n",
    "        self.count = 0\n",
    "        \n",
    "    def load_all_dataset(self):\n",
    "        # Add classes\n",
    "        for i, name in enumerate(self.label_names):\n",
    "            self.add_class(\"fashion\", i+1, name)\n",
    "            \n",
    "        # Add images and annotations\n",
    "        for i, row in self.photo_meta_db.iterrows():\n",
    "            \n",
    "            annIds = self.coco.getAnnIds(imgIds=row['id'], catIds=self.catIds, iscrowd=None)\n",
    "            anns = self.coco.loadAnns(annIds)\n",
    "            catids, polygons = [], []\n",
    "            for one in anns:\n",
    "                catids.append(one['category_id'])\n",
    "                polygons.extend(one['segmentation'])\n",
    "                \n",
    "\n",
    "            \n",
    "            if len(polygons) == 0:\n",
    "                continue\n",
    "            \n",
    "            fp = row['path'].split(\"?\")[0]\n",
    "            if False == fp.lower().endswith(\"jpg\") and False == fp.lower().endswith(\"jpeg\"):\n",
    "                continue\n",
    "                \n",
    "            self.add_image(\"fashion\", \n",
    "                           image_id = self.count,  #============> ID\n",
    "                           path = CURR_PATH + \"/data/paperdoll/images\" + fp ,\n",
    "                           labels = catids,\n",
    "                           polygons = polygons,\n",
    "                           height = row['height'], width=row['width'],\n",
    "                           annotations = self.coco.loadAnns(self.coco.getAnnIds(\n",
    "                            imgIds=row['id'], catIds=self.catIds, iscrowd=None)))\n",
    "\n",
    "        \n",
    "        print(\"load all data done, type :\", self.class_info)\n",
    "    \n",
    "    def annToRLE(self, ann, height, width):\n",
    "        \"\"\"\n",
    "        Convert annotation which can be polygons, uncompressed RLE to RLE.\n",
    "        :return: binary mask (numpy 2D array)\n",
    "        \"\"\"\n",
    "        segm = ann['segmentation']\n",
    "        if isinstance(segm, list):\n",
    "            rles = maskUtils.frPyObjects(segm, height, width)\n",
    "            rle = maskUtils.merge(rles)\n",
    "        elif isinstance(segm['counts'], list):\n",
    "            # uncompressed RLE\n",
    "            rle = maskUtils.frPyObjects(segm, height, width)\n",
    "        else:\n",
    "            # rle\n",
    "            rle = ann['segmentation']\n",
    "        return rle\n",
    "    \n",
    "    def annToMask(self, ann, height, width):\n",
    "        \"\"\"\n",
    "        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n",
    "        :return: binary mask (numpy 2D array)\n",
    "        \"\"\"\n",
    "        rle = self.annToRLE(ann, height, width)\n",
    "        m = maskUtils.decode(rle)\n",
    "        return m\n",
    "    \n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Load instance masks for the given image.\n",
    "\n",
    "        Different datasets use different ways to store masks. This\n",
    "        function converts the different mask format to one format\n",
    "        in the form of a bitmap [height, width, instances].\n",
    "\n",
    "        Returns:\n",
    "        masks: A bool array of shape [height, width, instance count] with\n",
    "            one mask per instance.\n",
    "        class_ids: a 1D array of class IDs of the instance masks.\n",
    "        \"\"\"\n",
    "        # If not a COCO image, delegate to parent class.\n",
    "        image_info = self.image_info[image_id]\n",
    "        \n",
    "        instance_masks = []\n",
    "        class_ids = []\n",
    "        annotations = self.image_info[image_id][\"annotations\"]\n",
    "        # Build mask of shape [height, width, instance_count] and list\n",
    "        # of class IDs that correspond to each channel of the mask.\n",
    "        for annotation in annotations:\n",
    "            class_id = self.map_source_class_id(\n",
    "                \"fashion.{}\".format(annotation['category_id']))\n",
    "            if class_id:\n",
    "                m = self.annToMask(annotation, image_info[\"height\"],\n",
    "                                   image_info[\"width\"])\n",
    "                # Some objects are so small that they're less than 1 pixel area\n",
    "                # and end up rounded out. Skip those objects.\n",
    "                if m.max() < 1:\n",
    "                    continue\n",
    "                # Is it a crowd? If so, use a negative class ID.\n",
    "                if annotation['iscrowd']:\n",
    "                    # Use negative class ID for crowds\n",
    "                    class_id *= -1\n",
    "                    # For crowd masks, annToMask() sometimes returns a mask\n",
    "                    # smaller than the given dimensions. If so, resize it.\n",
    "                    if m.shape[0] != image_info[\"height\"] or m.shape[1] != image_info[\"width\"]:\n",
    "                        m = np.ones([image_info[\"height\"], image_info[\"width\"]], dtype=bool)\n",
    "                instance_masks.append(m)\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "        # Pack instance masks into an array\n",
    "        if class_ids:\n",
    "            mask = np.stack(instance_masks, axis=2).astype(np.bool)\n",
    "            class_ids = np.array(class_ids, dtype=np.int32)\n",
    "            return mask, class_ids\n",
    "        else:\n",
    "            # Call super class to return an empty mask\n",
    "            return super(ModaNetDataset, self).load_mask(image_id)\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        return super(self.__class__, self).image_reference(image_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=4.96s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "####### Photo Meta Data\n",
    "access_str = \"file:\" + PAPERDOLL_META_DB + \"?mode=ro\"\n",
    "meta_db = sqlite3.connect(access_str, uri=True)\n",
    "photo_meta_db = pd.read_sql(\"\"\"\n",
    "            SELECT\n",
    "                *\n",
    "            FROM photos\n",
    "            WHERE photos.post_id IS NOT NULL AND file_file_size IS NOT NULL\n",
    "        \"\"\", con=meta_db)\n",
    "\n",
    "####### Image DataSet\n",
    "photo_data_set = PhotoData(PAPERDOLL_IMG_DB)\n",
    "\n",
    "\n",
    "####### Annotation Data\n",
    "coco = COCO(MODANET_ANNO)\n",
    "\n",
    "\n",
    "####### Model Config\n",
    "config = ModaNetConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "[{'supercategory': 'fashion', 'id': 1, 'name': 'bag'}, {'supercategory': 'fashion', 'id': 2, 'name': 'belt'}, {'supercategory': 'fashion', 'id': 3, 'name': 'boots'}, {'supercategory': 'fashion', 'id': 4, 'name': 'footwear'}, {'supercategory': 'fashion', 'id': 5, 'name': 'outer'}, {'supercategory': 'fashion', 'id': 6, 'name': 'dress'}, {'supercategory': 'fashion', 'id': 7, 'name': 'sunglasses'}, {'supercategory': 'fashion', 'id': 8, 'name': 'pants'}, {'supercategory': 'fashion', 'id': 9, 'name': 'top'}, {'supercategory': 'fashion', 'id': 10, 'name': 'shorts'}, {'supercategory': 'fashion', 'id': 11, 'name': 'skirt'}, {'supercategory': 'fashion', 'id': 12, 'name': 'headwear'}, {'supercategory': 'fashion', 'id': 13, 'name': 'scarf/tie'}]\n",
      "['bag', 'belt', 'boots', 'footwear', 'outer', 'dress', 'sunglasses', 'pants', 'top', 'shorts', 'skirt', 'headwear', 'scarf/tie']\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "total annotation number:  211828\n"
     ]
    }
   ],
   "source": [
    "print(coco.getCatIds())\n",
    "#id_set = coco.getImgIds(catIds=coco.getCatIds())\n",
    "cats = coco.loadCats(coco.getCatIds())\n",
    "print(cats)\n",
    "label_names = [cat['name'] for cat in cats]\n",
    "print(label_names)\n",
    "cat_ids = coco.getCatIds(catNms = label_names)\n",
    "print(cat_ids)\n",
    "\n",
    "anno_img_id_set = []\n",
    "for i in range(len(cat_ids)):\n",
    "    anno_img_id_set.extend(coco.getImgIds(catIds=[i]))\n",
    "print(\"total annotation number: \", len(anno_img_id_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo_meta_db size:  52254\n"
     ]
    }
   ],
   "source": [
    "# drop  images meta entries that without annotation\n",
    "anno_img_id_pd = pd.DataFrame({'id':anno_img_id_set})\n",
    "anno_img_id_pd['id'] =anno_img_id_pd['id'].apply(int)\n",
    "photo_meta_db = pd.merge(photo_meta_db, anno_img_id_pd, how='inner', on=['id'])\n",
    "# drop duplicate\n",
    "photo_meta_db = photo_meta_db.drop_duplicates()\n",
    "print(\"photo_meta_db size: \", len(photo_meta_db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id  post_id                                               path  status  \\\n",
      "0     3       73  /photos/thefashbot/3613535083/3420751783/34207...       2   \n",
      "4    13      142   /photos/thefashbot/2353856772/2353856772_400.jpg       2   \n",
      "10  148       79     /photos/poppylee/3083327112/3083327112_400.jpg       2   \n",
      "14  156      104     /photos/poppylee/8150270690/8150270690_400.jpg       2   \n",
      "17  174      353     /photos/Carolina/7809452194/7809452194_400.jpg       2   \n",
      "\n",
      "                    file_file_name  file_file_size file_content_type  \\\n",
      "0   open-uri20120907-29328-1uz77bi          115073        image/jpeg   \n",
      "4     open-uri20120907-29328-s7gmu          106894        image/jpeg   \n",
      "10  open-uri20120907-29328-1l7sf9u          124987        image/jpeg   \n",
      "14   open-uri20120907-29328-v7lh6v          126139        image/jpeg   \n",
      "17  open-uri20120907-29328-1x0oi3x          208504        image/jpeg   \n",
      "\n",
      "               file_updated_at  width  height                  created_at  \\\n",
      "0   2012-09-07 23:07:46.825511    400     600  2012-09-07 23:07:45.785651   \n",
      "4   2012-09-07 23:08:31.073256    400     600  2012-09-07 23:08:30.687548   \n",
      "10  2012-09-07 23:14:50.212592    400     600  2012-09-07 23:14:49.805666   \n",
      "14  2012-09-07 23:15:06.192652    400     600  2012-09-07 23:15:05.860767   \n",
      "17  2012-09-07 23:15:40.865098    400     600  2012-09-07 23:15:39.440233   \n",
      "\n",
      "                    updated_at  \n",
      "0   2012-09-07 23:07:47.722063  \n",
      "4   2012-09-07 23:08:32.031942  \n",
      "10  2012-09-07 23:14:50.742101  \n",
      "14  2012-09-07 23:15:06.622103  \n",
      "17  2012-09-07 23:15:41.682777  \n"
     ]
    }
   ],
   "source": [
    "print(photo_meta_db.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo_meta_db = photo_meta_db.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load all data done, type : [{'source': '', 'id': 0, 'name': 'BG'}, {'source': 'fashion', 'id': 1, 'name': 'bag'}, {'source': 'fashion', 'id': 2, 'name': 'belt'}, {'source': 'fashion', 'id': 3, 'name': 'boots'}, {'source': 'fashion', 'id': 4, 'name': 'footwear'}, {'source': 'fashion', 'id': 5, 'name': 'outer'}, {'source': 'fashion', 'id': 6, 'name': 'dress'}, {'source': 'fashion', 'id': 7, 'name': 'sunglasses'}, {'source': 'fashion', 'id': 8, 'name': 'pants'}, {'source': 'fashion', 'id': 9, 'name': 'top'}, {'source': 'fashion', 'id': 10, 'name': 'shorts'}, {'source': 'fashion', 'id': 11, 'name': 'skirt'}, {'source': 'fashion', 'id': 12, 'name': 'headwear'}, {'source': 'fashion', 'id': 13, 'name': 'scarf/tie'}]\n"
     ]
    }
   ],
   "source": [
    "dataset = ModaNetDataset(photo_meta_db, coco, label_names, cat_ids)\n",
    "dataset.load_all_dataset()\n",
    "dataset.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(600, 400, 7) [6 7 5 8 4 4 5] 7 7 10\n",
      "{'id': 0, 'source': 'fashion', 'path': '/root/workspace/product_image_segmentation/data/paperdoll/images/photos/toropiski/1905696948/9697010762/heather-gray-wool-dress-forest-green-leather-coat-black-tights_400.jpg', 'labels': [6, 7, 5, 8, 4, 4, 5], 'polygons': [[176, 140, 196, 144, 213, 148, 217, 147, 225, 159, 222, 171, 211, 192, 221, 199, 236, 194, 242, 191, 248, 215, 254, 238, 243, 246, 233, 246, 233, 277, 233, 291, 231, 312, 231, 331, 229, 347, 183, 344, 165, 344, 134, 342, 126, 342, 132, 316, 137, 257, 147, 233, 137, 237, 134, 224, 133, 209, 133, 197, 138, 184, 154, 175, 158, 190, 165, 197, 174, 187, 172, 176, 174, 168, 174, 158, 175, 147], [165, 104, 177, 104, 189, 103, 193, 103, 211, 102, 193, 106, 187, 113, 183, 115, 174, 109, 171, 115, 165, 110], [212, 126, 221, 128, 239, 143, 252, 151, 268, 159, 267, 165, 271, 178, 294, 255, 301, 299, 298, 307, 280, 313, 273, 314, 270, 318, 269, 392, 259, 395, 239, 390, 245, 370, 252, 262, 256, 240, 249, 215, 241, 190, 234, 180, 231, 172, 224, 171, 224, 158, 217, 137], [126, 152, 136, 144, 145, 139, 161, 134, 172, 128, 177, 127, 182, 137, 182, 142, 178, 140, 173, 161, 172, 171, 163, 171, 154, 175, 137, 185, 131, 199, 134, 223, 137, 238, 148, 234, 138, 257, 131, 318, 126, 344, 128, 349, 131, 379, 128, 386, 119, 383, 112, 385, 101, 385, 101, 380, 95, 374, 100, 300, 94, 297, 80, 293, 72, 287, 74, 280, 100, 212, 112, 181, 122, 158], [190, 345, 215, 347, 228, 347, 222, 377, 223, 391, 229, 422, 228, 458, 225, 478, 226, 499, 225, 508, 217, 516, 207, 515, 203, 508, 203, 501, 209, 492, 208, 480, 210, 471, 209, 454, 203, 439, 199, 410, 191, 388, 189, 360], [129, 342, 137, 341, 165, 343, 159, 361, 161, 383, 151, 406, 146, 429, 136, 454, 134, 471, 133, 486, 129, 505, 123, 509, 115, 506, 115, 499, 117, 488, 117, 474, 120, 462, 115, 429, 122, 398, 131, 378, 129, 348], [134, 473, 136, 485, 132, 505, 128, 526, 120, 526, 109, 525, 110, 514, 110, 506, 115, 499, 115, 506, 124, 509, 129, 507, 133, 487, 133, 483], [224, 506, 224, 523, 220, 533, 211, 536, 201, 535, 200, 527, 200, 515, 203, 509, 207, 515, 215, 516, 221, 513], [224, 380, 236, 381, 242, 379, 245, 366, 252, 260, 256, 239, 252, 236, 245, 245, 234, 245, 233, 277, 231, 336, 228, 348, 222, 378], [163, 369, 165, 373, 178, 375, 189, 373, 190, 358, 190, 343, 164, 344, 158, 361, 161, 373]], 'height': 600, 'width': 400, 'annotations': [{'segmentation': [[176, 140, 196, 144, 213, 148, 217, 147, 225, 159, 222, 171, 211, 192, 221, 199, 236, 194, 242, 191, 248, 215, 254, 238, 243, 246, 233, 246, 233, 277, 233, 291, 231, 312, 231, 331, 229, 347, 183, 344, 165, 344, 134, 342, 126, 342, 132, 316, 137, 257, 147, 233, 137, 237, 134, 224, 133, 209, 133, 197, 138, 184, 154, 175, 158, 190, 165, 197, 174, 187, 172, 176, 174, 168, 174, 158, 175, 147]], 'area': 26496, 'iscrowd': 0, 'image_id': 370874, 'bbox': [126, 140, 128, 207], 'category_id': 6, 'id': 94378}, {'segmentation': [[165, 104, 177, 104, 189, 103, 193, 103, 211, 102, 193, 106, 187, 113, 183, 115, 174, 109, 171, 115, 165, 110]], 'area': 598, 'iscrowd': 0, 'image_id': 370874, 'bbox': [165, 102, 46, 13], 'category_id': 7, 'id': 94379}, {'segmentation': [[212, 126, 221, 128, 239, 143, 252, 151, 268, 159, 267, 165, 271, 178, 294, 255, 301, 299, 298, 307, 280, 313, 273, 314, 270, 318, 269, 392, 259, 395, 239, 390, 245, 370, 252, 262, 256, 240, 249, 215, 241, 190, 234, 180, 231, 172, 224, 171, 224, 158, 217, 137], [126, 152, 136, 144, 145, 139, 161, 134, 172, 128, 177, 127, 182, 137, 182, 142, 178, 140, 173, 161, 172, 171, 163, 171, 154, 175, 137, 185, 131, 199, 134, 223, 137, 238, 148, 234, 138, 257, 131, 318, 126, 344, 128, 349, 131, 379, 128, 386, 119, 383, 112, 385, 101, 385, 101, 380, 95, 374, 100, 300, 94, 297, 80, 293, 72, 287, 74, 280, 100, 212, 112, 181, 122, 158]], 'area': 61601, 'iscrowd': 0, 'image_id': 370874, 'bbox': [72, 126, 229, 269], 'category_id': 5, 'id': 94380}, {'segmentation': [[190, 345, 215, 347, 228, 347, 222, 377, 223, 391, 229, 422, 228, 458, 225, 478, 226, 499, 225, 508, 217, 516, 207, 515, 203, 508, 203, 501, 209, 492, 208, 480, 210, 471, 209, 454, 203, 439, 199, 410, 191, 388, 189, 360], [129, 342, 137, 341, 165, 343, 159, 361, 161, 383, 151, 406, 146, 429, 136, 454, 134, 471, 133, 486, 129, 505, 123, 509, 115, 506, 115, 499, 117, 488, 117, 474, 120, 462, 115, 429, 122, 398, 131, 378, 129, 348]], 'area': 19950, 'iscrowd': 0, 'image_id': 370874, 'bbox': [115, 341, 114, 175], 'category_id': 8, 'id': 94381}, {'segmentation': [[134, 473, 136, 485, 132, 505, 128, 526, 120, 526, 109, 525, 110, 514, 110, 506, 115, 499, 115, 506, 124, 509, 129, 507, 133, 487, 133, 483]], 'area': 1431, 'iscrowd': 0, 'image_id': 370874, 'bbox': [200, 506, 27, 53], 'category_id': 4, 'id': 94382}, {'segmentation': [[224, 506, 224, 523, 220, 533, 211, 536, 201, 535, 200, 527, 200, 515, 203, 509, 207, 515, 215, 516, 221, 513]], 'area': 720, 'iscrowd': 0, 'image_id': 370874, 'bbox': [200, 506, 24, 30], 'category_id': 4, 'id': 94383}, {'segmentation': [[224, 380, 236, 381, 242, 379, 245, 366, 252, 260, 256, 239, 252, 236, 245, 245, 234, 245, 233, 277, 231, 336, 228, 348, 222, 378], [163, 369, 165, 373, 178, 375, 189, 373, 190, 358, 190, 343, 164, 344, 158, 361, 161, 373]], 'area': 14210, 'iscrowd': 0, 'image_id': 370874, 'bbox': [158, 236, 98, 145], 'category_id': 5, 'id': 94384}]}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x144 with 11 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(600, 400, 5) [ 2  9 11  4  4] 5 5 5\n",
      "{'id': 0, 'source': 'fashion', 'path': '/root/workspace/product_image_segmentation/data/paperdoll/images/photos/missmatchgirrrl/11253773498/carrot-orange-socks-heather-gray-sweater-black-forever-21-shoes-ivory-neck_400.jpg', 'labels': [2, 9, 11, 4, 4], 'polygons': [[171, 277, 186, 274, 204, 274, 211, 277, 230, 277, 244, 277, 249, 278, 251, 281, 233, 284, 208, 284, 196, 282, 182, 280], [177, 188, 163, 194, 158, 205, 153, 235, 146, 258, 139, 266, 135, 277, 126, 301, 120, 310, 134, 316, 140, 308, 158, 280, 165, 282, 172, 273, 189, 272, 217, 276, 242, 276, 250, 280, 256, 299, 256, 324, 262, 323, 273, 299, 285, 290, 298, 273, 331, 298, 340, 287, 316, 263, 290, 229, 266, 198, 256, 188, 247, 184, 228, 193, 228, 207, 224, 219, 223, 229, 220, 233, 222, 248, 215, 253, 207, 250, 206, 231, 205, 216, 199, 202, 199, 190, 194, 197], [169, 280, 153, 301, 137, 329, 136, 339, 119, 371, 116, 388, 99, 422, 110, 433, 111, 440, 120, 447, 137, 445, 139, 451, 151, 453, 160, 445, 165, 444, 167, 461, 187, 460, 205, 456, 208, 436, 216, 443, 239, 444, 249, 445, 251, 434, 259, 434, 266, 425, 266, 414, 273, 403, 261, 352, 259, 325, 256, 323, 254, 298, 249, 280, 232, 284, 208, 284, 175, 279], [191, 528, 194, 554, 195, 559, 207, 560, 215, 558, 218, 548, 212, 525], [118, 502, 114, 517, 103, 519, 95, 517, 86, 537, 80, 552, 77, 558, 78, 561, 88, 562, 99, 555, 107, 546, 113, 528, 117, 528, 124, 520, 124, 513]], 'height': 600, 'width': 400, 'annotations': [{'segmentation': [[171, 277, 186, 274, 204, 274, 211, 277, 230, 277, 244, 277, 249, 278, 251, 281, 233, 284, 208, 284, 196, 282, 182, 280]], 'area': 800, 'iscrowd': 0, 'image_id': 468298, 'bbox': [171, 274, 80, 10], 'category_id': 2, 'id': 147930}, {'segmentation': [[177, 188, 163, 194, 158, 205, 153, 235, 146, 258, 139, 266, 135, 277, 126, 301, 120, 310, 134, 316, 140, 308, 158, 280, 165, 282, 172, 273, 189, 272, 217, 276, 242, 276, 250, 280, 256, 299, 256, 324, 262, 323, 273, 299, 285, 290, 298, 273, 331, 298, 340, 287, 316, 263, 290, 229, 266, 198, 256, 188, 247, 184, 228, 193, 228, 207, 224, 219, 223, 229, 220, 233, 222, 248, 215, 253, 207, 250, 206, 231, 205, 216, 199, 202, 199, 190, 194, 197]], 'area': 30800, 'iscrowd': 0, 'image_id': 468298, 'bbox': [120, 184, 220, 140], 'category_id': 9, 'id': 147931}, {'segmentation': [[169, 280, 153, 301, 137, 329, 136, 339, 119, 371, 116, 388, 99, 422, 110, 433, 111, 440, 120, 447, 137, 445, 139, 451, 151, 453, 160, 445, 165, 444, 167, 461, 187, 460, 205, 456, 208, 436, 216, 443, 239, 444, 249, 445, 251, 434, 259, 434, 266, 425, 266, 414, 273, 403, 261, 352, 259, 325, 256, 323, 254, 298, 249, 280, 232, 284, 208, 284, 175, 279]], 'area': 31668, 'iscrowd': 0, 'image_id': 468298, 'bbox': [99, 279, 174, 182], 'category_id': 11, 'id': 147932}, {'segmentation': [[191, 528, 194, 554, 195, 559, 207, 560, 215, 558, 218, 548, 212, 525]], 'area': 945, 'iscrowd': 0, 'image_id': 468298, 'bbox': [77, 502, 27, 35], 'category_id': 4, 'id': 147933}, {'segmentation': [[118, 502, 114, 517, 103, 519, 95, 517, 86, 537, 80, 552, 77, 558, 78, 561, 88, 562, 99, 555, 107, 546, 113, 528, 117, 528, 124, 520, 124, 513]], 'area': 2820, 'iscrowd': 0, 'image_id': 468298, 'bbox': [77, 502, 47, 60], 'category_id': 4, 'id': 147934}]}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x144 with 11 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(600, 400, 5) [ 2 13  9  5 11] 5 5 7\n",
      "{'id': 0, 'source': 'fashion', 'path': '/root/workspace/product_image_segmentation/data/paperdoll/images/photos/Losteardrop/6659329746/6659329746_400.jpg', 'labels': [2, 13, 9, 5, 11], 'polygons': [[235, 324, 228, 325, 225, 332, 206, 339, 191, 342, 181, 341, 174, 342, 160, 334, 152, 363, 177, 370, 194, 373, 209, 369, 191, 417, 174, 449, 174, 459, 183, 467, 193, 462, 205, 453, 229, 400, 236, 377, 237, 364, 245, 364, 239, 330, 236, 322], [189, 156, 182, 152, 169, 157, 167, 155, 159, 159, 170, 179, 164, 200, 163, 248, 167, 312, 171, 329, 214, 325, 199, 227, 187, 196, 181, 186, 189, 156], [167, 144, 153, 129, 147, 161, 133, 169, 158, 303, 156, 328, 173, 330, 167, 312, 162, 247, 165, 202, 171, 180, 159, 159, 167, 154, 167, 144], [196, 122, 179, 141, 168, 146, 169, 153, 185, 153, 190, 158, 182, 188, 187, 198, 199, 226, 214, 326, 235, 316, 220, 195, 214, 183, 216, 177, 211, 165, 200, 152, 198, 147, 198, 129, 197, 125], [116, 172, 103, 172, 91, 151, 73, 158, 60, 180, 65, 282, 65, 393, 61, 428, 55, 431, 67, 453, 70, 486, 87, 483, 106, 479, 127, 488, 155, 366, 160, 327, 154, 324, 158, 305, 133, 171, 116, 175], [245, 118, 265, 124, 282, 146, 288, 172, 305, 226, 317, 261, 319, 292, 298, 319, 287, 319, 268, 309, 292, 392, 300, 437, 281, 464, 264, 472, 245, 365, 236, 314, 221, 193, 215, 184, 216, 177, 229, 178, 250, 161, 250, 145, 232, 113, 246, 119], [237, 534, 202, 540, 168, 538, 146, 535, 115, 518, 111, 483, 131, 487, 155, 365, 179, 371, 191, 374, 208, 372, 190, 421, 174, 449, 174, 459, 184, 467, 196, 460, 203, 453, 227, 397, 237, 377, 237, 361, 245, 367, 264, 471, 280, 465, 289, 481, 284, 492, 271, 513, 248, 522, 240, 534]], 'height': 600, 'width': 400, 'annotations': [{'segmentation': [[235, 324, 228, 325, 225, 332, 206, 339, 191, 342, 181, 341, 174, 342, 160, 334, 152, 363, 177, 370, 194, 373, 209, 369, 191, 417, 174, 449, 174, 459, 183, 467, 193, 462, 205, 453, 229, 400, 236, 377, 237, 364, 245, 364, 239, 330, 236, 322]], 'area': 13485, 'iscrowd': 0, 'image_id': 185424, 'bbox': [152, 322, 93, 145], 'category_id': 2, 'id': 109739}, {'segmentation': [[189, 156, 182, 152, 169, 157, 167, 155, 159, 159, 170, 179, 164, 200, 163, 248, 167, 312, 171, 329, 214, 325, 199, 227, 187, 196, 181, 186, 189, 156]], 'area': 9735, 'iscrowd': 0, 'image_id': 185424, 'bbox': [159, 152, 55, 177], 'category_id': 13, 'id': 109740}, {'segmentation': [[167, 144, 153, 129, 147, 161, 133, 169, 158, 303, 156, 328, 173, 330, 167, 312, 162, 247, 165, 202, 171, 180, 159, 159, 167, 154, 167, 144], [196, 122, 179, 141, 168, 146, 169, 153, 185, 153, 190, 158, 182, 188, 187, 198, 199, 226, 214, 326, 235, 316, 220, 195, 214, 183, 216, 177, 211, 165, 200, 152, 198, 147, 198, 129, 197, 125]], 'area': 21216, 'iscrowd': 0, 'image_id': 185424, 'bbox': [133, 122, 102, 208], 'category_id': 9, 'id': 109741}, {'segmentation': [[116, 172, 103, 172, 91, 151, 73, 158, 60, 180, 65, 282, 65, 393, 61, 428, 55, 431, 67, 453, 70, 486, 87, 483, 106, 479, 127, 488, 155, 366, 160, 327, 154, 324, 158, 305, 133, 171, 116, 175], [245, 118, 265, 124, 282, 146, 288, 172, 305, 226, 317, 261, 319, 292, 298, 319, 287, 319, 268, 309, 292, 392, 300, 437, 281, 464, 264, 472, 245, 365, 236, 314, 221, 193, 215, 184, 216, 177, 229, 178, 250, 161, 250, 145, 232, 113, 246, 119]], 'area': 99000, 'iscrowd': 0, 'image_id': 185424, 'bbox': [55, 113, 264, 375], 'category_id': 5, 'id': 109742}, {'segmentation': [[237, 534, 202, 540, 168, 538, 146, 535, 115, 518, 111, 483, 131, 487, 155, 365, 179, 371, 191, 374, 208, 372, 190, 421, 174, 449, 174, 459, 184, 467, 196, 460, 203, 453, 227, 397, 237, 377, 237, 361, 245, 367, 264, 471, 280, 465, 289, 481, 284, 492, 271, 513, 248, 522, 240, 534]], 'area': 31862, 'iscrowd': 0, 'image_id': 185424, 'bbox': [111, 361, 178, 179], 'category_id': 11, 'id': 109743}]}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x144 with 11 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CHECK THE DATABASE\n",
    "for i in range(3):\n",
    "    image_id = random.choice(dataset.image_ids)\n",
    "    print(dataset.image_reference(image_id))\n",
    "    \n",
    "    image = dataset.load_image(image_id)\n",
    "    mask, class_ids = dataset.load_mask(image_id)\n",
    "\n",
    "    print(mask.shape, class_ids ,len(class_ids), len(dataset.image_info[image_id]['labels']),  len(dataset.image_info[image_id]['polygons']))\n",
    "    print(dataset.image_info[image_id])\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset.class_names, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category distribution: \n",
      "1 20570\n",
      "2 14295\n",
      "3 7084\n",
      "4 39222\n",
      "5 23691\n",
      "6 14426\n",
      "7 8792\n",
      "8 23015\n",
      "9 34681\n",
      "10 7031\n",
      "11 13575\n",
      "12 5446\n",
      "13 5020\n",
      "216848\n"
     ]
    }
   ],
   "source": [
    "print(\"category distribution: \")\n",
    "total_num = 0\n",
    "for i in cat_ids:\n",
    "    print(i, len(coco.getImgIds(catIds=[i])))\n",
    "    total_num += len(coco.getImgIds(catIds=[i]))\n",
    "    \n",
    "print(total_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size:  41803\n",
      "valid set size:  10451\n",
      "load all data done, type : [{'source': '', 'id': 0, 'name': 'BG'}, {'source': 'fashion', 'id': 1, 'name': 'bag'}, {'source': 'fashion', 'id': 2, 'name': 'belt'}, {'source': 'fashion', 'id': 3, 'name': 'boots'}, {'source': 'fashion', 'id': 4, 'name': 'footwear'}, {'source': 'fashion', 'id': 5, 'name': 'outer'}, {'source': 'fashion', 'id': 6, 'name': 'dress'}, {'source': 'fashion', 'id': 7, 'name': 'sunglasses'}, {'source': 'fashion', 'id': 8, 'name': 'pants'}, {'source': 'fashion', 'id': 9, 'name': 'top'}, {'source': 'fashion', 'id': 10, 'name': 'shorts'}, {'source': 'fashion', 'id': 11, 'name': 'skirt'}, {'source': 'fashion', 'id': 12, 'name': 'headwear'}, {'source': 'fashion', 'id': 13, 'name': 'scarf/tie'}]\n",
      "load all data done, type : [{'source': '', 'id': 0, 'name': 'BG'}, {'source': 'fashion', 'id': 1, 'name': 'bag'}, {'source': 'fashion', 'id': 2, 'name': 'belt'}, {'source': 'fashion', 'id': 3, 'name': 'boots'}, {'source': 'fashion', 'id': 4, 'name': 'footwear'}, {'source': 'fashion', 'id': 5, 'name': 'outer'}, {'source': 'fashion', 'id': 6, 'name': 'dress'}, {'source': 'fashion', 'id': 7, 'name': 'sunglasses'}, {'source': 'fashion', 'id': 8, 'name': 'pants'}, {'source': 'fashion', 'id': 9, 'name': 'top'}, {'source': 'fashion', 'id': 10, 'name': 'shorts'}, {'source': 'fashion', 'id': 11, 'name': 'skirt'}, {'source': 'fashion', 'id': 12, 'name': 'headwear'}, {'source': 'fashion', 'id': 13, 'name': 'scarf/tie'}]\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=N_FOLDS, shuffle=True)\n",
    "splits = kf.split(photo_meta_db) \n",
    "\n",
    "def get_fold(splits, train_df, idx):    \n",
    "    for i, (train_index, valid_index) in enumerate(splits):\n",
    "        if i == idx:\n",
    "            return train_df.iloc[train_index], train_df.iloc[valid_index]\n",
    "        \n",
    "train_df, valid_df = get_fold(splits, photo_meta_db, SELECT_FOLD_IDX)\n",
    "\n",
    "print(\"train set size: \" , len(train_df))\n",
    "print(\"valid set size: \" , len(valid_df))\n",
    "\n",
    "train_dataset = ModaNetDataset(train_df, coco, label_names, cat_ids)\n",
    "train_dataset.load_all_dataset()\n",
    "train_dataset.prepare()\n",
    "\n",
    "\n",
    "valid_dataset = ModaNetDataset(valid_df, coco, label_names, cat_ids)\n",
    "valid_dataset.load_all_dataset()\n",
    "valid_dataset.prepare()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-starting from epoch 7\n"
     ]
    }
   ],
   "source": [
    "model = modellib.MaskRCNN(mode=\"training\", config=config, model_dir=LOG_DIR)   \n",
    "model_path =\\\n",
    "    \"/root/workspace/product_image_segmentation/weight_files/logs/modanet20190718T1840/mask_rcnn_modanet_0007.h5\"\n",
    "model.load_weights(model_path, by_name=True, \n",
    "                   exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\"mrcnn_bbox\", \"mrcnn_mask\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_fold(splits, train_df, idx):    \n",
    "    for i, (train_index, valid_index) in enumerate(splits):\n",
    "        if i == idx:\n",
    "            return train_df.iloc[train_index], train_df.iloc[valid_index]\n",
    "        \n",
    "\n",
    "def split_dateset(photo_meta_db, N_FOLDS, coco, label_names, cat_ids):\n",
    "    selected = random.randint(0,10000)%N_FOLDS\n",
    "\n",
    "    kf = KFold(n_splits=N_FOLDS, shuffle=True)\n",
    "    splits = kf.split(photo_meta_db) \n",
    " \n",
    "    train_set, valid_set = get_fold(splits, photo_meta_db, selected)\n",
    "\n",
    "\n",
    "    train_dataset = ModaNetDataset(train_set, coco, label_names, cat_ids)\n",
    "    train_dataset.load_all_dataset()\n",
    "    train_dataset.prepare()\n",
    "    print(\"train set size: \" , len(train_set), type(train_dataset))\n",
    "\n",
    "    valid_dataset = ModaNetDataset(valid_set, coco, label_names, cat_ids)\n",
    "    valid_dataset.load_all_dataset()\n",
    "    valid_dataset.prepare()\n",
    "    print(\"valid set size: \" , len(valid_set), type(valid_dataset))\n",
    "    \n",
    "    return train_dataset, valid_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load all data done, type : [{'source': '', 'id': 0, 'name': 'BG'}, {'source': 'fashion', 'id': 1, 'name': 'bag'}, {'source': 'fashion', 'id': 2, 'name': 'belt'}, {'source': 'fashion', 'id': 3, 'name': 'boots'}, {'source': 'fashion', 'id': 4, 'name': 'footwear'}, {'source': 'fashion', 'id': 5, 'name': 'outer'}, {'source': 'fashion', 'id': 6, 'name': 'dress'}, {'source': 'fashion', 'id': 7, 'name': 'sunglasses'}, {'source': 'fashion', 'id': 8, 'name': 'pants'}, {'source': 'fashion', 'id': 9, 'name': 'top'}, {'source': 'fashion', 'id': 10, 'name': 'shorts'}, {'source': 'fashion', 'id': 11, 'name': 'skirt'}, {'source': 'fashion', 'id': 12, 'name': 'headwear'}, {'source': 'fashion', 'id': 13, 'name': 'scarf/tie'}]\n",
      "train set size:  41804 <class '__main__.ModaNetDataset'>\n",
      "load all data done, type : [{'source': '', 'id': 0, 'name': 'BG'}, {'source': 'fashion', 'id': 1, 'name': 'bag'}, {'source': 'fashion', 'id': 2, 'name': 'belt'}, {'source': 'fashion', 'id': 3, 'name': 'boots'}, {'source': 'fashion', 'id': 4, 'name': 'footwear'}, {'source': 'fashion', 'id': 5, 'name': 'outer'}, {'source': 'fashion', 'id': 6, 'name': 'dress'}, {'source': 'fashion', 'id': 7, 'name': 'sunglasses'}, {'source': 'fashion', 'id': 8, 'name': 'pants'}, {'source': 'fashion', 'id': 9, 'name': 'top'}, {'source': 'fashion', 'id': 10, 'name': 'shorts'}, {'source': 'fashion', 'id': 11, 'name': 'skirt'}, {'source': 'fashion', 'id': 12, 'name': 'headwear'}, {'source': 'fashion', 'id': 13, 'name': 'scarf/tie'}]\n",
      "valid set size:  10450 <class '__main__.ModaNetDataset'>\n",
      "\n",
      "Starting at epoch 7. LR=0.002\n",
      "\n",
      "Checkpoint Path: /root/workspace/product_image_segmentation/weight_files/logs/modanet20190718T1840/mask_rcnn_modanet_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "conv1                  (Conv2D)\n",
      "bn_conv1               (BatchNorm)\n",
      "res2a_branch2a         (Conv2D)\n",
      "bn2a_branch2a          (BatchNorm)\n",
      "res2a_branch2b         (Conv2D)\n",
      "bn2a_branch2b          (BatchNorm)\n",
      "res2a_branch2c         (Conv2D)\n",
      "res2a_branch1          (Conv2D)\n",
      "bn2a_branch2c          (BatchNorm)\n",
      "bn2a_branch1           (BatchNorm)\n",
      "res2b_branch2a         (Conv2D)\n",
      "bn2b_branch2a          (BatchNorm)\n",
      "res2b_branch2b         (Conv2D)\n",
      "bn2b_branch2b          (BatchNorm)\n",
      "res2b_branch2c         (Conv2D)\n",
      "bn2b_branch2c          (BatchNorm)\n",
      "res2c_branch2a         (Conv2D)\n",
      "bn2c_branch2a          (BatchNorm)\n",
      "res2c_branch2b         (Conv2D)\n",
      "bn2c_branch2b          (BatchNorm)\n",
      "res2c_branch2c         (Conv2D)\n",
      "bn2c_branch2c          (BatchNorm)\n",
      "res3a_branch2a         (Conv2D)\n",
      "bn3a_branch2a          (BatchNorm)\n",
      "res3a_branch2b         (Conv2D)\n",
      "bn3a_branch2b          (BatchNorm)\n",
      "res3a_branch2c         (Conv2D)\n",
      "res3a_branch1          (Conv2D)\n",
      "bn3a_branch2c          (BatchNorm)\n",
      "bn3a_branch1           (BatchNorm)\n",
      "res3b_branch2a         (Conv2D)\n",
      "bn3b_branch2a          (BatchNorm)\n",
      "res3b_branch2b         (Conv2D)\n",
      "bn3b_branch2b          (BatchNorm)\n",
      "res3b_branch2c         (Conv2D)\n",
      "bn3b_branch2c          (BatchNorm)\n",
      "res3c_branch2a         (Conv2D)\n",
      "bn3c_branch2a          (BatchNorm)\n",
      "res3c_branch2b         (Conv2D)\n",
      "bn3c_branch2b          (BatchNorm)\n",
      "res3c_branch2c         (Conv2D)\n",
      "bn3c_branch2c          (BatchNorm)\n",
      "res3d_branch2a         (Conv2D)\n",
      "bn3d_branch2a          (BatchNorm)\n",
      "res3d_branch2b         (Conv2D)\n",
      "bn3d_branch2b          (BatchNorm)\n",
      "res3d_branch2c         (Conv2D)\n",
      "bn3d_branch2c          (BatchNorm)\n",
      "res4a_branch2a         (Conv2D)\n",
      "bn4a_branch2a          (BatchNorm)\n",
      "res4a_branch2b         (Conv2D)\n",
      "bn4a_branch2b          (BatchNorm)\n",
      "res4a_branch2c         (Conv2D)\n",
      "res4a_branch1          (Conv2D)\n",
      "bn4a_branch2c          (BatchNorm)\n",
      "bn4a_branch1           (BatchNorm)\n",
      "res4b_branch2a         (Conv2D)\n",
      "bn4b_branch2a          (BatchNorm)\n",
      "res4b_branch2b         (Conv2D)\n",
      "bn4b_branch2b          (BatchNorm)\n",
      "res4b_branch2c         (Conv2D)\n",
      "bn4b_branch2c          (BatchNorm)\n",
      "res4c_branch2a         (Conv2D)\n",
      "bn4c_branch2a          (BatchNorm)\n",
      "res4c_branch2b         (Conv2D)\n",
      "bn4c_branch2b          (BatchNorm)\n",
      "res4c_branch2c         (Conv2D)\n",
      "bn4c_branch2c          (BatchNorm)\n",
      "res4d_branch2a         (Conv2D)\n",
      "bn4d_branch2a          (BatchNorm)\n",
      "res4d_branch2b         (Conv2D)\n",
      "bn4d_branch2b          (BatchNorm)\n",
      "res4d_branch2c         (Conv2D)\n",
      "bn4d_branch2c          (BatchNorm)\n",
      "res4e_branch2a         (Conv2D)\n",
      "bn4e_branch2a          (BatchNorm)\n",
      "res4e_branch2b         (Conv2D)\n",
      "bn4e_branch2b          (BatchNorm)\n",
      "res4e_branch2c         (Conv2D)\n",
      "bn4e_branch2c          (BatchNorm)\n",
      "res4f_branch2a         (Conv2D)\n",
      "bn4f_branch2a          (BatchNorm)\n",
      "res4f_branch2b         (Conv2D)\n",
      "bn4f_branch2b          (BatchNorm)\n",
      "res4f_branch2c         (Conv2D)\n",
      "bn4f_branch2c          (BatchNorm)\n",
      "res5a_branch2a         (Conv2D)\n",
      "bn5a_branch2a          (BatchNorm)\n",
      "res5a_branch2b         (Conv2D)\n",
      "bn5a_branch2b          (BatchNorm)\n",
      "res5a_branch2c         (Conv2D)\n",
      "res5a_branch1          (Conv2D)\n",
      "bn5a_branch2c          (BatchNorm)\n",
      "bn5a_branch1           (BatchNorm)\n",
      "res5b_branch2a         (Conv2D)\n",
      "bn5b_branch2a          (BatchNorm)\n",
      "res5b_branch2b         (Conv2D)\n",
      "bn5b_branch2b          (BatchNorm)\n",
      "res5b_branch2c         (Conv2D)\n",
      "bn5b_branch2c          (BatchNorm)\n",
      "res5c_branch2a         (Conv2D)\n",
      "bn5c_branch2a          (BatchNorm)\n",
      "res5c_branch2b         (Conv2D)\n",
      "bn5c_branch2b          (BatchNorm)\n",
      "res5c_branch2c         (Conv2D)\n",
      "bn5c_branch2c          (BatchNorm)\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/8\n",
      "1000/1000 [==============================] - 2769s 3s/step - loss: 1.1903 - rpn_class_loss: 0.0201 - rpn_bbox_loss: 0.3448 - mrcnn_class_loss: 0.3202 - mrcnn_bbox_loss: 0.2173 - mrcnn_mask_loss: 0.2879 - val_loss: 1.4447 - val_rpn_class_loss: 0.0217 - val_rpn_bbox_loss: 0.5336 - val_mrcnn_class_loss: 0.3251 - val_mrcnn_bbox_loss: 0.2755 - val_mrcnn_mask_loss: 0.2887\n",
      "One epoch done, NO.: 8  LR: 20\n",
      "load all data done, type : [{'source': '', 'id': 0, 'name': 'BG'}, {'source': 'fashion', 'id': 1, 'name': 'bag'}, {'source': 'fashion', 'id': 2, 'name': 'belt'}, {'source': 'fashion', 'id': 3, 'name': 'boots'}, {'source': 'fashion', 'id': 4, 'name': 'footwear'}, {'source': 'fashion', 'id': 5, 'name': 'outer'}, {'source': 'fashion', 'id': 6, 'name': 'dress'}, {'source': 'fashion', 'id': 7, 'name': 'sunglasses'}, {'source': 'fashion', 'id': 8, 'name': 'pants'}, {'source': 'fashion', 'id': 9, 'name': 'top'}, {'source': 'fashion', 'id': 10, 'name': 'shorts'}, {'source': 'fashion', 'id': 11, 'name': 'skirt'}, {'source': 'fashion', 'id': 12, 'name': 'headwear'}, {'source': 'fashion', 'id': 13, 'name': 'scarf/tie'}]\n",
      "train set size:  41803 <class '__main__.ModaNetDataset'>\n",
      "load all data done, type : [{'source': '', 'id': 0, 'name': 'BG'}, {'source': 'fashion', 'id': 1, 'name': 'bag'}, {'source': 'fashion', 'id': 2, 'name': 'belt'}, {'source': 'fashion', 'id': 3, 'name': 'boots'}, {'source': 'fashion', 'id': 4, 'name': 'footwear'}, {'source': 'fashion', 'id': 5, 'name': 'outer'}, {'source': 'fashion', 'id': 6, 'name': 'dress'}, {'source': 'fashion', 'id': 7, 'name': 'sunglasses'}, {'source': 'fashion', 'id': 8, 'name': 'pants'}, {'source': 'fashion', 'id': 9, 'name': 'top'}, {'source': 'fashion', 'id': 10, 'name': 'shorts'}, {'source': 'fashion', 'id': 11, 'name': 'skirt'}, {'source': 'fashion', 'id': 12, 'name': 'headwear'}, {'source': 'fashion', 'id': 13, 'name': 'scarf/tie'}]\n",
      "valid set size:  10451 <class '__main__.ModaNetDataset'>\n",
      "\n",
      "Starting at epoch 8. LR=0.0015\n",
      "\n",
      "Checkpoint Path: /root/workspace/product_image_segmentation/weight_files/logs/modanet20190718T1840/mask_rcnn_modanet_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "conv1                  (Conv2D)\n",
      "bn_conv1               (BatchNorm)\n",
      "res2a_branch2a         (Conv2D)\n",
      "bn2a_branch2a          (BatchNorm)\n",
      "res2a_branch2b         (Conv2D)\n",
      "bn2a_branch2b          (BatchNorm)\n",
      "res2a_branch2c         (Conv2D)\n",
      "res2a_branch1          (Conv2D)\n",
      "bn2a_branch2c          (BatchNorm)\n",
      "bn2a_branch1           (BatchNorm)\n",
      "res2b_branch2a         (Conv2D)\n",
      "bn2b_branch2a          (BatchNorm)\n",
      "res2b_branch2b         (Conv2D)\n",
      "bn2b_branch2b          (BatchNorm)\n",
      "res2b_branch2c         (Conv2D)\n",
      "bn2b_branch2c          (BatchNorm)\n",
      "res2c_branch2a         (Conv2D)\n",
      "bn2c_branch2a          (BatchNorm)\n",
      "res2c_branch2b         (Conv2D)\n",
      "bn2c_branch2b          (BatchNorm)\n",
      "res2c_branch2c         (Conv2D)\n",
      "bn2c_branch2c          (BatchNorm)\n",
      "res3a_branch2a         (Conv2D)\n",
      "bn3a_branch2a          (BatchNorm)\n",
      "res3a_branch2b         (Conv2D)\n",
      "bn3a_branch2b          (BatchNorm)\n",
      "res3a_branch2c         (Conv2D)\n",
      "res3a_branch1          (Conv2D)\n",
      "bn3a_branch2c          (BatchNorm)\n",
      "bn3a_branch1           (BatchNorm)\n",
      "res3b_branch2a         (Conv2D)\n",
      "bn3b_branch2a          (BatchNorm)\n",
      "res3b_branch2b         (Conv2D)\n",
      "bn3b_branch2b          (BatchNorm)\n",
      "res3b_branch2c         (Conv2D)\n",
      "bn3b_branch2c          (BatchNorm)\n",
      "res3c_branch2a         (Conv2D)\n",
      "bn3c_branch2a          (BatchNorm)\n",
      "res3c_branch2b         (Conv2D)\n",
      "bn3c_branch2b          (BatchNorm)\n",
      "res3c_branch2c         (Conv2D)\n",
      "bn3c_branch2c          (BatchNorm)\n",
      "res3d_branch2a         (Conv2D)\n",
      "bn3d_branch2a          (BatchNorm)\n",
      "res3d_branch2b         (Conv2D)\n",
      "bn3d_branch2b          (BatchNorm)\n",
      "res3d_branch2c         (Conv2D)\n",
      "bn3d_branch2c          (BatchNorm)\n",
      "res4a_branch2a         (Conv2D)\n",
      "bn4a_branch2a          (BatchNorm)\n",
      "res4a_branch2b         (Conv2D)\n",
      "bn4a_branch2b          (BatchNorm)\n",
      "res4a_branch2c         (Conv2D)\n",
      "res4a_branch1          (Conv2D)\n",
      "bn4a_branch2c          (BatchNorm)\n",
      "bn4a_branch1           (BatchNorm)\n",
      "res4b_branch2a         (Conv2D)\n",
      "bn4b_branch2a          (BatchNorm)\n",
      "res4b_branch2b         (Conv2D)\n",
      "bn4b_branch2b          (BatchNorm)\n",
      "res4b_branch2c         (Conv2D)\n",
      "bn4b_branch2c          (BatchNorm)\n",
      "res4c_branch2a         (Conv2D)\n",
      "bn4c_branch2a          (BatchNorm)\n",
      "res4c_branch2b         (Conv2D)\n",
      "bn4c_branch2b          (BatchNorm)\n",
      "res4c_branch2c         (Conv2D)\n",
      "bn4c_branch2c          (BatchNorm)\n",
      "res4d_branch2a         (Conv2D)\n",
      "bn4d_branch2a          (BatchNorm)\n",
      "res4d_branch2b         (Conv2D)\n",
      "bn4d_branch2b          (BatchNorm)\n",
      "res4d_branch2c         (Conv2D)\n",
      "bn4d_branch2c          (BatchNorm)\n",
      "res4e_branch2a         (Conv2D)\n",
      "bn4e_branch2a          (BatchNorm)\n",
      "res4e_branch2b         (Conv2D)\n",
      "bn4e_branch2b          (BatchNorm)\n",
      "res4e_branch2c         (Conv2D)\n",
      "bn4e_branch2c          (BatchNorm)\n",
      "res4f_branch2a         (Conv2D)\n",
      "bn4f_branch2a          (BatchNorm)\n",
      "res4f_branch2b         (Conv2D)\n",
      "bn4f_branch2b          (BatchNorm)\n",
      "res4f_branch2c         (Conv2D)\n",
      "bn4f_branch2c          (BatchNorm)\n",
      "res5a_branch2a         (Conv2D)\n",
      "bn5a_branch2a          (BatchNorm)\n",
      "res5a_branch2b         (Conv2D)\n",
      "bn5a_branch2b          (BatchNorm)\n",
      "res5a_branch2c         (Conv2D)\n",
      "res5a_branch1          (Conv2D)\n",
      "bn5a_branch2c          (BatchNorm)\n",
      "bn5a_branch1           (BatchNorm)\n",
      "res5b_branch2a         (Conv2D)\n",
      "bn5b_branch2a          (BatchNorm)\n",
      "res5b_branch2b         (Conv2D)\n",
      "bn5b_branch2b          (BatchNorm)\n",
      "res5b_branch2c         (Conv2D)\n",
      "bn5b_branch2c          (BatchNorm)\n",
      "res5c_branch2a         (Conv2D)\n",
      "bn5c_branch2a          (BatchNorm)\n",
      "res5c_branch2b         (Conv2D)\n",
      "bn5c_branch2b          (BatchNorm)\n",
      "res5c_branch2c         (Conv2D)\n",
      "bn5c_branch2c          (BatchNorm)\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/9\n",
      "1000/1000 [==============================] - 2796s 3s/step - loss: 1.0879 - rpn_class_loss: 0.0186 - rpn_bbox_loss: 0.2782 - mrcnn_class_loss: 0.3193 - mrcnn_bbox_loss: 0.2016 - mrcnn_mask_loss: 0.2703 - val_loss: 1.7671 - val_rpn_class_loss: 0.0307 - val_rpn_bbox_loss: 0.6272 - val_mrcnn_class_loss: 0.5185 - val_mrcnn_bbox_loss: 0.2914 - val_mrcnn_mask_loss: 0.2993\n",
      "One epoch done, NO.: 9  LR: 15\n",
      "load all data done, type : [{'source': '', 'id': 0, 'name': 'BG'}, {'source': 'fashion', 'id': 1, 'name': 'bag'}, {'source': 'fashion', 'id': 2, 'name': 'belt'}, {'source': 'fashion', 'id': 3, 'name': 'boots'}, {'source': 'fashion', 'id': 4, 'name': 'footwear'}, {'source': 'fashion', 'id': 5, 'name': 'outer'}, {'source': 'fashion', 'id': 6, 'name': 'dress'}, {'source': 'fashion', 'id': 7, 'name': 'sunglasses'}, {'source': 'fashion', 'id': 8, 'name': 'pants'}, {'source': 'fashion', 'id': 9, 'name': 'top'}, {'source': 'fashion', 'id': 10, 'name': 'shorts'}, {'source': 'fashion', 'id': 11, 'name': 'skirt'}, {'source': 'fashion', 'id': 12, 'name': 'headwear'}, {'source': 'fashion', 'id': 13, 'name': 'scarf/tie'}]\n",
      "train set size:  41804 <class '__main__.ModaNetDataset'>\n",
      "load all data done, type : [{'source': '', 'id': 0, 'name': 'BG'}, {'source': 'fashion', 'id': 1, 'name': 'bag'}, {'source': 'fashion', 'id': 2, 'name': 'belt'}, {'source': 'fashion', 'id': 3, 'name': 'boots'}, {'source': 'fashion', 'id': 4, 'name': 'footwear'}, {'source': 'fashion', 'id': 5, 'name': 'outer'}, {'source': 'fashion', 'id': 6, 'name': 'dress'}, {'source': 'fashion', 'id': 7, 'name': 'sunglasses'}, {'source': 'fashion', 'id': 8, 'name': 'pants'}, {'source': 'fashion', 'id': 9, 'name': 'top'}, {'source': 'fashion', 'id': 10, 'name': 'shorts'}, {'source': 'fashion', 'id': 11, 'name': 'skirt'}, {'source': 'fashion', 'id': 12, 'name': 'headwear'}, {'source': 'fashion', 'id': 13, 'name': 'scarf/tie'}]\n",
      "valid set size:  10450 <class '__main__.ModaNetDataset'>\n",
      "\n",
      "Starting at epoch 9. LR=0.0005\n",
      "\n",
      "Checkpoint Path: /root/workspace/product_image_segmentation/weight_files/logs/modanet20190718T1840/mask_rcnn_modanet_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "conv1                  (Conv2D)\n",
      "bn_conv1               (BatchNorm)\n",
      "res2a_branch2a         (Conv2D)\n",
      "bn2a_branch2a          (BatchNorm)\n",
      "res2a_branch2b         (Conv2D)\n",
      "bn2a_branch2b          (BatchNorm)\n",
      "res2a_branch2c         (Conv2D)\n",
      "res2a_branch1          (Conv2D)\n",
      "bn2a_branch2c          (BatchNorm)\n",
      "bn2a_branch1           (BatchNorm)\n",
      "res2b_branch2a         (Conv2D)\n",
      "bn2b_branch2a          (BatchNorm)\n",
      "res2b_branch2b         (Conv2D)\n",
      "bn2b_branch2b          (BatchNorm)\n",
      "res2b_branch2c         (Conv2D)\n",
      "bn2b_branch2c          (BatchNorm)\n",
      "res2c_branch2a         (Conv2D)\n",
      "bn2c_branch2a          (BatchNorm)\n",
      "res2c_branch2b         (Conv2D)\n",
      "bn2c_branch2b          (BatchNorm)\n",
      "res2c_branch2c         (Conv2D)\n",
      "bn2c_branch2c          (BatchNorm)\n",
      "res3a_branch2a         (Conv2D)\n",
      "bn3a_branch2a          (BatchNorm)\n",
      "res3a_branch2b         (Conv2D)\n",
      "bn3a_branch2b          (BatchNorm)\n",
      "res3a_branch2c         (Conv2D)\n",
      "res3a_branch1          (Conv2D)\n",
      "bn3a_branch2c          (BatchNorm)\n",
      "bn3a_branch1           (BatchNorm)\n",
      "res3b_branch2a         (Conv2D)\n",
      "bn3b_branch2a          (BatchNorm)\n",
      "res3b_branch2b         (Conv2D)\n",
      "bn3b_branch2b          (BatchNorm)\n",
      "res3b_branch2c         (Conv2D)\n",
      "bn3b_branch2c          (BatchNorm)\n",
      "res3c_branch2a         (Conv2D)\n",
      "bn3c_branch2a          (BatchNorm)\n",
      "res3c_branch2b         (Conv2D)\n",
      "bn3c_branch2b          (BatchNorm)\n",
      "res3c_branch2c         (Conv2D)\n",
      "bn3c_branch2c          (BatchNorm)\n",
      "res3d_branch2a         (Conv2D)\n",
      "bn3d_branch2a          (BatchNorm)\n",
      "res3d_branch2b         (Conv2D)\n",
      "bn3d_branch2b          (BatchNorm)\n",
      "res3d_branch2c         (Conv2D)\n",
      "bn3d_branch2c          (BatchNorm)\n",
      "res4a_branch2a         (Conv2D)\n",
      "bn4a_branch2a          (BatchNorm)\n",
      "res4a_branch2b         (Conv2D)\n",
      "bn4a_branch2b          (BatchNorm)\n",
      "res4a_branch2c         (Conv2D)\n",
      "res4a_branch1          (Conv2D)\n",
      "bn4a_branch2c          (BatchNorm)\n",
      "bn4a_branch1           (BatchNorm)\n",
      "res4b_branch2a         (Conv2D)\n",
      "bn4b_branch2a          (BatchNorm)\n",
      "res4b_branch2b         (Conv2D)\n",
      "bn4b_branch2b          (BatchNorm)\n",
      "res4b_branch2c         (Conv2D)\n",
      "bn4b_branch2c          (BatchNorm)\n",
      "res4c_branch2a         (Conv2D)\n",
      "bn4c_branch2a          (BatchNorm)\n",
      "res4c_branch2b         (Conv2D)\n",
      "bn4c_branch2b          (BatchNorm)\n",
      "res4c_branch2c         (Conv2D)\n",
      "bn4c_branch2c          (BatchNorm)\n",
      "res4d_branch2a         (Conv2D)\n",
      "bn4d_branch2a          (BatchNorm)\n",
      "res4d_branch2b         (Conv2D)\n",
      "bn4d_branch2b          (BatchNorm)\n",
      "res4d_branch2c         (Conv2D)\n",
      "bn4d_branch2c          (BatchNorm)\n",
      "res4e_branch2a         (Conv2D)\n",
      "bn4e_branch2a          (BatchNorm)\n",
      "res4e_branch2b         (Conv2D)\n",
      "bn4e_branch2b          (BatchNorm)\n",
      "res4e_branch2c         (Conv2D)\n",
      "bn4e_branch2c          (BatchNorm)\n",
      "res4f_branch2a         (Conv2D)\n",
      "bn4f_branch2a          (BatchNorm)\n",
      "res4f_branch2b         (Conv2D)\n",
      "bn4f_branch2b          (BatchNorm)\n",
      "res4f_branch2c         (Conv2D)\n",
      "bn4f_branch2c          (BatchNorm)\n",
      "res5a_branch2a         (Conv2D)\n",
      "bn5a_branch2a          (BatchNorm)\n",
      "res5a_branch2b         (Conv2D)\n",
      "bn5a_branch2b          (BatchNorm)\n",
      "res5a_branch2c         (Conv2D)\n",
      "res5a_branch1          (Conv2D)\n",
      "bn5a_branch2c          (BatchNorm)\n",
      "bn5a_branch1           (BatchNorm)\n",
      "res5b_branch2a         (Conv2D)\n",
      "bn5b_branch2a          (BatchNorm)\n",
      "res5b_branch2b         (Conv2D)\n",
      "bn5b_branch2b          (BatchNorm)\n",
      "res5b_branch2c         (Conv2D)\n",
      "bn5b_branch2c          (BatchNorm)\n",
      "res5c_branch2a         (Conv2D)\n",
      "bn5c_branch2a          (BatchNorm)\n",
      "res5c_branch2b         (Conv2D)\n",
      "bn5c_branch2b          (BatchNorm)\n",
      "res5c_branch2c         (Conv2D)\n",
      "bn5c_branch2c          (BatchNorm)\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 2779s 3s/step - loss: 1.1644 - rpn_class_loss: 0.0209 - rpn_bbox_loss: 0.3140 - mrcnn_class_loss: 0.3632 - mrcnn_bbox_loss: 0.2105 - mrcnn_mask_loss: 0.2559 - val_loss: 1.4295 - val_rpn_class_loss: 0.0288 - val_rpn_bbox_loss: 0.4207 - val_mrcnn_class_loss: 0.4405 - val_mrcnn_bbox_loss: 0.2521 - val_mrcnn_mask_loss: 0.2874\n",
      "One epoch done, NO.: 10  LR: 5\n",
      "load all data done, type : [{'source': '', 'id': 0, 'name': 'BG'}, {'source': 'fashion', 'id': 1, 'name': 'bag'}, {'source': 'fashion', 'id': 2, 'name': 'belt'}, {'source': 'fashion', 'id': 3, 'name': 'boots'}, {'source': 'fashion', 'id': 4, 'name': 'footwear'}, {'source': 'fashion', 'id': 5, 'name': 'outer'}, {'source': 'fashion', 'id': 6, 'name': 'dress'}, {'source': 'fashion', 'id': 7, 'name': 'sunglasses'}, {'source': 'fashion', 'id': 8, 'name': 'pants'}, {'source': 'fashion', 'id': 9, 'name': 'top'}, {'source': 'fashion', 'id': 10, 'name': 'shorts'}, {'source': 'fashion', 'id': 11, 'name': 'skirt'}, {'source': 'fashion', 'id': 12, 'name': 'headwear'}, {'source': 'fashion', 'id': 13, 'name': 'scarf/tie'}]\n",
      "train set size:  41804 <class '__main__.ModaNetDataset'>\n",
      "load all data done, type : [{'source': '', 'id': 0, 'name': 'BG'}, {'source': 'fashion', 'id': 1, 'name': 'bag'}, {'source': 'fashion', 'id': 2, 'name': 'belt'}, {'source': 'fashion', 'id': 3, 'name': 'boots'}, {'source': 'fashion', 'id': 4, 'name': 'footwear'}, {'source': 'fashion', 'id': 5, 'name': 'outer'}, {'source': 'fashion', 'id': 6, 'name': 'dress'}, {'source': 'fashion', 'id': 7, 'name': 'sunglasses'}, {'source': 'fashion', 'id': 8, 'name': 'pants'}, {'source': 'fashion', 'id': 9, 'name': 'top'}, {'source': 'fashion', 'id': 10, 'name': 'shorts'}, {'source': 'fashion', 'id': 11, 'name': 'skirt'}, {'source': 'fashion', 'id': 12, 'name': 'headwear'}, {'source': 'fashion', 'id': 13, 'name': 'scarf/tie'}]\n",
      "valid set size:  10450 <class '__main__.ModaNetDataset'>\n",
      "\n",
      "Starting at epoch 10. LR=0.001\n",
      "\n",
      "Checkpoint Path: /root/workspace/product_image_segmentation/weight_files/logs/modanet20190718T1840/mask_rcnn_modanet_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "conv1                  (Conv2D)\n",
      "bn_conv1               (BatchNorm)\n",
      "res2a_branch2a         (Conv2D)\n",
      "bn2a_branch2a          (BatchNorm)\n",
      "res2a_branch2b         (Conv2D)\n",
      "bn2a_branch2b          (BatchNorm)\n",
      "res2a_branch2c         (Conv2D)\n",
      "res2a_branch1          (Conv2D)\n",
      "bn2a_branch2c          (BatchNorm)\n",
      "bn2a_branch1           (BatchNorm)\n",
      "res2b_branch2a         (Conv2D)\n",
      "bn2b_branch2a          (BatchNorm)\n",
      "res2b_branch2b         (Conv2D)\n",
      "bn2b_branch2b          (BatchNorm)\n",
      "res2b_branch2c         (Conv2D)\n",
      "bn2b_branch2c          (BatchNorm)\n",
      "res2c_branch2a         (Conv2D)\n",
      "bn2c_branch2a          (BatchNorm)\n",
      "res2c_branch2b         (Conv2D)\n",
      "bn2c_branch2b          (BatchNorm)\n",
      "res2c_branch2c         (Conv2D)\n",
      "bn2c_branch2c          (BatchNorm)\n",
      "res3a_branch2a         (Conv2D)\n",
      "bn3a_branch2a          (BatchNorm)\n",
      "res3a_branch2b         (Conv2D)\n",
      "bn3a_branch2b          (BatchNorm)\n",
      "res3a_branch2c         (Conv2D)\n",
      "res3a_branch1          (Conv2D)\n",
      "bn3a_branch2c          (BatchNorm)\n",
      "bn3a_branch1           (BatchNorm)\n",
      "res3b_branch2a         (Conv2D)\n",
      "bn3b_branch2a          (BatchNorm)\n",
      "res3b_branch2b         (Conv2D)\n",
      "bn3b_branch2b          (BatchNorm)\n",
      "res3b_branch2c         (Conv2D)\n",
      "bn3b_branch2c          (BatchNorm)\n",
      "res3c_branch2a         (Conv2D)\n",
      "bn3c_branch2a          (BatchNorm)\n",
      "res3c_branch2b         (Conv2D)\n",
      "bn3c_branch2b          (BatchNorm)\n",
      "res3c_branch2c         (Conv2D)\n",
      "bn3c_branch2c          (BatchNorm)\n",
      "res3d_branch2a         (Conv2D)\n",
      "bn3d_branch2a          (BatchNorm)\n",
      "res3d_branch2b         (Conv2D)\n",
      "bn3d_branch2b          (BatchNorm)\n",
      "res3d_branch2c         (Conv2D)\n",
      "bn3d_branch2c          (BatchNorm)\n",
      "res4a_branch2a         (Conv2D)\n",
      "bn4a_branch2a          (BatchNorm)\n",
      "res4a_branch2b         (Conv2D)\n",
      "bn4a_branch2b          (BatchNorm)\n",
      "res4a_branch2c         (Conv2D)\n",
      "res4a_branch1          (Conv2D)\n",
      "bn4a_branch2c          (BatchNorm)\n",
      "bn4a_branch1           (BatchNorm)\n",
      "res4b_branch2a         (Conv2D)\n",
      "bn4b_branch2a          (BatchNorm)\n",
      "res4b_branch2b         (Conv2D)\n",
      "bn4b_branch2b          (BatchNorm)\n",
      "res4b_branch2c         (Conv2D)\n",
      "bn4b_branch2c          (BatchNorm)\n",
      "res4c_branch2a         (Conv2D)\n",
      "bn4c_branch2a          (BatchNorm)\n",
      "res4c_branch2b         (Conv2D)\n",
      "bn4c_branch2b          (BatchNorm)\n",
      "res4c_branch2c         (Conv2D)\n",
      "bn4c_branch2c          (BatchNorm)\n",
      "res4d_branch2a         (Conv2D)\n",
      "bn4d_branch2a          (BatchNorm)\n",
      "res4d_branch2b         (Conv2D)\n",
      "bn4d_branch2b          (BatchNorm)\n",
      "res4d_branch2c         (Conv2D)\n",
      "bn4d_branch2c          (BatchNorm)\n",
      "res4e_branch2a         (Conv2D)\n",
      "bn4e_branch2a          (BatchNorm)\n",
      "res4e_branch2b         (Conv2D)\n",
      "bn4e_branch2b          (BatchNorm)\n",
      "res4e_branch2c         (Conv2D)\n",
      "bn4e_branch2c          (BatchNorm)\n",
      "res4f_branch2a         (Conv2D)\n",
      "bn4f_branch2a          (BatchNorm)\n",
      "res4f_branch2b         (Conv2D)\n",
      "bn4f_branch2b          (BatchNorm)\n",
      "res4f_branch2c         (Conv2D)\n",
      "bn4f_branch2c          (BatchNorm)\n",
      "res5a_branch2a         (Conv2D)\n",
      "bn5a_branch2a          (BatchNorm)\n",
      "res5a_branch2b         (Conv2D)\n",
      "bn5a_branch2b          (BatchNorm)\n",
      "res5a_branch2c         (Conv2D)\n",
      "res5a_branch1          (Conv2D)\n",
      "bn5a_branch2c          (BatchNorm)\n",
      "bn5a_branch1           (BatchNorm)\n",
      "res5b_branch2a         (Conv2D)\n",
      "bn5b_branch2a          (BatchNorm)\n",
      "res5b_branch2b         (Conv2D)\n",
      "bn5b_branch2b          (BatchNorm)\n",
      "res5b_branch2c         (Conv2D)\n",
      "bn5b_branch2c          (BatchNorm)\n",
      "res5c_branch2a         (Conv2D)\n",
      "bn5c_branch2a          (BatchNorm)\n",
      "res5c_branch2b         (Conv2D)\n",
      "bn5c_branch2b          (BatchNorm)\n",
      "res5c_branch2c         (Conv2D)\n",
      "bn5c_branch2c          (BatchNorm)\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/11\n",
      "1000/1000 [==============================] - 2860s 3s/step - loss: 1.0703 - rpn_class_loss: 0.0214 - rpn_bbox_loss: 0.2938 - mrcnn_class_loss: 0.3060 - mrcnn_bbox_loss: 0.1955 - mrcnn_mask_loss: 0.2536 - val_loss: 1.5468 - val_rpn_class_loss: 0.0448 - val_rpn_bbox_loss: 0.5982 - val_mrcnn_class_loss: 0.3814 - val_mrcnn_bbox_loss: 0.2345 - val_mrcnn_mask_loss: 0.2878\n",
      "One epoch done, NO.: 11  LR: 10\n",
      "load all data done, type : [{'source': '', 'id': 0, 'name': 'BG'}, {'source': 'fashion', 'id': 1, 'name': 'bag'}, {'source': 'fashion', 'id': 2, 'name': 'belt'}, {'source': 'fashion', 'id': 3, 'name': 'boots'}, {'source': 'fashion', 'id': 4, 'name': 'footwear'}, {'source': 'fashion', 'id': 5, 'name': 'outer'}, {'source': 'fashion', 'id': 6, 'name': 'dress'}, {'source': 'fashion', 'id': 7, 'name': 'sunglasses'}, {'source': 'fashion', 'id': 8, 'name': 'pants'}, {'source': 'fashion', 'id': 9, 'name': 'top'}, {'source': 'fashion', 'id': 10, 'name': 'shorts'}, {'source': 'fashion', 'id': 11, 'name': 'skirt'}, {'source': 'fashion', 'id': 12, 'name': 'headwear'}, {'source': 'fashion', 'id': 13, 'name': 'scarf/tie'}]\n",
      "train set size:  41803 <class '__main__.ModaNetDataset'>\n",
      "load all data done, type : [{'source': '', 'id': 0, 'name': 'BG'}, {'source': 'fashion', 'id': 1, 'name': 'bag'}, {'source': 'fashion', 'id': 2, 'name': 'belt'}, {'source': 'fashion', 'id': 3, 'name': 'boots'}, {'source': 'fashion', 'id': 4, 'name': 'footwear'}, {'source': 'fashion', 'id': 5, 'name': 'outer'}, {'source': 'fashion', 'id': 6, 'name': 'dress'}, {'source': 'fashion', 'id': 7, 'name': 'sunglasses'}, {'source': 'fashion', 'id': 8, 'name': 'pants'}, {'source': 'fashion', 'id': 9, 'name': 'top'}, {'source': 'fashion', 'id': 10, 'name': 'shorts'}, {'source': 'fashion', 'id': 11, 'name': 'skirt'}, {'source': 'fashion', 'id': 12, 'name': 'headwear'}, {'source': 'fashion', 'id': 13, 'name': 'scarf/tie'}]\n",
      "valid set size:  10451 <class '__main__.ModaNetDataset'>\n",
      "\n",
      "Starting at epoch 11. LR=0.00030000000000000003\n",
      "\n",
      "Checkpoint Path: /root/workspace/product_image_segmentation/weight_files/logs/modanet20190718T1840/mask_rcnn_modanet_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "conv1                  (Conv2D)\n",
      "bn_conv1               (BatchNorm)\n",
      "res2a_branch2a         (Conv2D)\n",
      "bn2a_branch2a          (BatchNorm)\n",
      "res2a_branch2b         (Conv2D)\n",
      "bn2a_branch2b          (BatchNorm)\n",
      "res2a_branch2c         (Conv2D)\n",
      "res2a_branch1          (Conv2D)\n",
      "bn2a_branch2c          (BatchNorm)\n",
      "bn2a_branch1           (BatchNorm)\n",
      "res2b_branch2a         (Conv2D)\n",
      "bn2b_branch2a          (BatchNorm)\n",
      "res2b_branch2b         (Conv2D)\n",
      "bn2b_branch2b          (BatchNorm)\n",
      "res2b_branch2c         (Conv2D)\n",
      "bn2b_branch2c          (BatchNorm)\n",
      "res2c_branch2a         (Conv2D)\n",
      "bn2c_branch2a          (BatchNorm)\n",
      "res2c_branch2b         (Conv2D)\n",
      "bn2c_branch2b          (BatchNorm)\n",
      "res2c_branch2c         (Conv2D)\n",
      "bn2c_branch2c          (BatchNorm)\n",
      "res3a_branch2a         (Conv2D)\n",
      "bn3a_branch2a          (BatchNorm)\n",
      "res3a_branch2b         (Conv2D)\n",
      "bn3a_branch2b          (BatchNorm)\n",
      "res3a_branch2c         (Conv2D)\n",
      "res3a_branch1          (Conv2D)\n",
      "bn3a_branch2c          (BatchNorm)\n",
      "bn3a_branch1           (BatchNorm)\n",
      "res3b_branch2a         (Conv2D)\n",
      "bn3b_branch2a          (BatchNorm)\n",
      "res3b_branch2b         (Conv2D)\n",
      "bn3b_branch2b          (BatchNorm)\n",
      "res3b_branch2c         (Conv2D)\n",
      "bn3b_branch2c          (BatchNorm)\n",
      "res3c_branch2a         (Conv2D)\n",
      "bn3c_branch2a          (BatchNorm)\n",
      "res3c_branch2b         (Conv2D)\n",
      "bn3c_branch2b          (BatchNorm)\n",
      "res3c_branch2c         (Conv2D)\n",
      "bn3c_branch2c          (BatchNorm)\n",
      "res3d_branch2a         (Conv2D)\n",
      "bn3d_branch2a          (BatchNorm)\n",
      "res3d_branch2b         (Conv2D)\n",
      "bn3d_branch2b          (BatchNorm)\n",
      "res3d_branch2c         (Conv2D)\n",
      "bn3d_branch2c          (BatchNorm)\n",
      "res4a_branch2a         (Conv2D)\n",
      "bn4a_branch2a          (BatchNorm)\n",
      "res4a_branch2b         (Conv2D)\n",
      "bn4a_branch2b          (BatchNorm)\n",
      "res4a_branch2c         (Conv2D)\n",
      "res4a_branch1          (Conv2D)\n",
      "bn4a_branch2c          (BatchNorm)\n",
      "bn4a_branch1           (BatchNorm)\n",
      "res4b_branch2a         (Conv2D)\n",
      "bn4b_branch2a          (BatchNorm)\n",
      "res4b_branch2b         (Conv2D)\n",
      "bn4b_branch2b          (BatchNorm)\n",
      "res4b_branch2c         (Conv2D)\n",
      "bn4b_branch2c          (BatchNorm)\n",
      "res4c_branch2a         (Conv2D)\n",
      "bn4c_branch2a          (BatchNorm)\n",
      "res4c_branch2b         (Conv2D)\n",
      "bn4c_branch2b          (BatchNorm)\n",
      "res4c_branch2c         (Conv2D)\n",
      "bn4c_branch2c          (BatchNorm)\n",
      "res4d_branch2a         (Conv2D)\n",
      "bn4d_branch2a          (BatchNorm)\n",
      "res4d_branch2b         (Conv2D)\n",
      "bn4d_branch2b          (BatchNorm)\n",
      "res4d_branch2c         (Conv2D)\n",
      "bn4d_branch2c          (BatchNorm)\n",
      "res4e_branch2a         (Conv2D)\n",
      "bn4e_branch2a          (BatchNorm)\n",
      "res4e_branch2b         (Conv2D)\n",
      "bn4e_branch2b          (BatchNorm)\n",
      "res4e_branch2c         (Conv2D)\n",
      "bn4e_branch2c          (BatchNorm)\n",
      "res4f_branch2a         (Conv2D)\n",
      "bn4f_branch2a          (BatchNorm)\n",
      "res4f_branch2b         (Conv2D)\n",
      "bn4f_branch2b          (BatchNorm)\n",
      "res4f_branch2c         (Conv2D)\n",
      "bn4f_branch2c          (BatchNorm)\n",
      "res5a_branch2a         (Conv2D)\n",
      "bn5a_branch2a          (BatchNorm)\n",
      "res5a_branch2b         (Conv2D)\n",
      "bn5a_branch2b          (BatchNorm)\n",
      "res5a_branch2c         (Conv2D)\n",
      "res5a_branch1          (Conv2D)\n",
      "bn5a_branch2c          (BatchNorm)\n",
      "bn5a_branch1           (BatchNorm)\n",
      "res5b_branch2a         (Conv2D)\n",
      "bn5b_branch2a          (BatchNorm)\n",
      "res5b_branch2b         (Conv2D)\n",
      "bn5b_branch2b          (BatchNorm)\n",
      "res5b_branch2c         (Conv2D)\n",
      "bn5b_branch2c          (BatchNorm)\n",
      "res5c_branch2a         (Conv2D)\n",
      "bn5c_branch2a          (BatchNorm)\n",
      "res5c_branch2b         (Conv2D)\n",
      "bn5c_branch2b          (BatchNorm)\n",
      "res5c_branch2c         (Conv2D)\n",
      "bn5c_branch2c          (BatchNorm)\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 2851s 3s/step - loss: 1.1509 - rpn_class_loss: 0.0198 - rpn_bbox_loss: 0.3085 - mrcnn_class_loss: 0.3507 - mrcnn_bbox_loss: 0.2088 - mrcnn_mask_loss: 0.2630 - val_loss: 1.2130 - val_rpn_class_loss: 0.0135 - val_rpn_bbox_loss: 0.2877 - val_mrcnn_class_loss: 0.4208 - val_mrcnn_bbox_loss: 0.2118 - val_mrcnn_mask_loss: 0.2792\n",
      "One epoch done, NO.: 12  LR: 3\n",
      "load all data done, type : [{'source': '', 'id': 0, 'name': 'BG'}, {'source': 'fashion', 'id': 1, 'name': 'bag'}, {'source': 'fashion', 'id': 2, 'name': 'belt'}, {'source': 'fashion', 'id': 3, 'name': 'boots'}, {'source': 'fashion', 'id': 4, 'name': 'footwear'}, {'source': 'fashion', 'id': 5, 'name': 'outer'}, {'source': 'fashion', 'id': 6, 'name': 'dress'}, {'source': 'fashion', 'id': 7, 'name': 'sunglasses'}, {'source': 'fashion', 'id': 8, 'name': 'pants'}, {'source': 'fashion', 'id': 9, 'name': 'top'}, {'source': 'fashion', 'id': 10, 'name': 'shorts'}, {'source': 'fashion', 'id': 11, 'name': 'skirt'}, {'source': 'fashion', 'id': 12, 'name': 'headwear'}, {'source': 'fashion', 'id': 13, 'name': 'scarf/tie'}]\n",
      "train set size:  41803 <class '__main__.ModaNetDataset'>\n",
      "load all data done, type : [{'source': '', 'id': 0, 'name': 'BG'}, {'source': 'fashion', 'id': 1, 'name': 'bag'}, {'source': 'fashion', 'id': 2, 'name': 'belt'}, {'source': 'fashion', 'id': 3, 'name': 'boots'}, {'source': 'fashion', 'id': 4, 'name': 'footwear'}, {'source': 'fashion', 'id': 5, 'name': 'outer'}, {'source': 'fashion', 'id': 6, 'name': 'dress'}, {'source': 'fashion', 'id': 7, 'name': 'sunglasses'}, {'source': 'fashion', 'id': 8, 'name': 'pants'}, {'source': 'fashion', 'id': 9, 'name': 'top'}, {'source': 'fashion', 'id': 10, 'name': 'shorts'}, {'source': 'fashion', 'id': 11, 'name': 'skirt'}, {'source': 'fashion', 'id': 12, 'name': 'headwear'}, {'source': 'fashion', 'id': 13, 'name': 'scarf/tie'}]\n",
      "valid set size:  10451 <class '__main__.ModaNetDataset'>\n",
      "\n",
      "Starting at epoch 12. LR=0.0005\n",
      "\n",
      "Checkpoint Path: /root/workspace/product_image_segmentation/weight_files/logs/modanet20190718T1840/mask_rcnn_modanet_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "conv1                  (Conv2D)\n",
      "bn_conv1               (BatchNorm)\n",
      "res2a_branch2a         (Conv2D)\n",
      "bn2a_branch2a          (BatchNorm)\n",
      "res2a_branch2b         (Conv2D)\n",
      "bn2a_branch2b          (BatchNorm)\n",
      "res2a_branch2c         (Conv2D)\n",
      "res2a_branch1          (Conv2D)\n",
      "bn2a_branch2c          (BatchNorm)\n",
      "bn2a_branch1           (BatchNorm)\n",
      "res2b_branch2a         (Conv2D)\n",
      "bn2b_branch2a          (BatchNorm)\n",
      "res2b_branch2b         (Conv2D)\n",
      "bn2b_branch2b          (BatchNorm)\n",
      "res2b_branch2c         (Conv2D)\n",
      "bn2b_branch2c          (BatchNorm)\n",
      "res2c_branch2a         (Conv2D)\n",
      "bn2c_branch2a          (BatchNorm)\n",
      "res2c_branch2b         (Conv2D)\n",
      "bn2c_branch2b          (BatchNorm)\n",
      "res2c_branch2c         (Conv2D)\n",
      "bn2c_branch2c          (BatchNorm)\n",
      "res3a_branch2a         (Conv2D)\n",
      "bn3a_branch2a          (BatchNorm)\n",
      "res3a_branch2b         (Conv2D)\n",
      "bn3a_branch2b          (BatchNorm)\n",
      "res3a_branch2c         (Conv2D)\n",
      "res3a_branch1          (Conv2D)\n",
      "bn3a_branch2c          (BatchNorm)\n",
      "bn3a_branch1           (BatchNorm)\n",
      "res3b_branch2a         (Conv2D)\n",
      "bn3b_branch2a          (BatchNorm)\n",
      "res3b_branch2b         (Conv2D)\n",
      "bn3b_branch2b          (BatchNorm)\n",
      "res3b_branch2c         (Conv2D)\n",
      "bn3b_branch2c          (BatchNorm)\n",
      "res3c_branch2a         (Conv2D)\n",
      "bn3c_branch2a          (BatchNorm)\n",
      "res3c_branch2b         (Conv2D)\n",
      "bn3c_branch2b          (BatchNorm)\n",
      "res3c_branch2c         (Conv2D)\n",
      "bn3c_branch2c          (BatchNorm)\n",
      "res3d_branch2a         (Conv2D)\n",
      "bn3d_branch2a          (BatchNorm)\n",
      "res3d_branch2b         (Conv2D)\n",
      "bn3d_branch2b          (BatchNorm)\n",
      "res3d_branch2c         (Conv2D)\n",
      "bn3d_branch2c          (BatchNorm)\n",
      "res4a_branch2a         (Conv2D)\n",
      "bn4a_branch2a          (BatchNorm)\n",
      "res4a_branch2b         (Conv2D)\n",
      "bn4a_branch2b          (BatchNorm)\n",
      "res4a_branch2c         (Conv2D)\n",
      "res4a_branch1          (Conv2D)\n",
      "bn4a_branch2c          (BatchNorm)\n",
      "bn4a_branch1           (BatchNorm)\n",
      "res4b_branch2a         (Conv2D)\n",
      "bn4b_branch2a          (BatchNorm)\n",
      "res4b_branch2b         (Conv2D)\n",
      "bn4b_branch2b          (BatchNorm)\n",
      "res4b_branch2c         (Conv2D)\n",
      "bn4b_branch2c          (BatchNorm)\n",
      "res4c_branch2a         (Conv2D)\n",
      "bn4c_branch2a          (BatchNorm)\n",
      "res4c_branch2b         (Conv2D)\n",
      "bn4c_branch2b          (BatchNorm)\n",
      "res4c_branch2c         (Conv2D)\n",
      "bn4c_branch2c          (BatchNorm)\n",
      "res4d_branch2a         (Conv2D)\n",
      "bn4d_branch2a          (BatchNorm)\n",
      "res4d_branch2b         (Conv2D)\n",
      "bn4d_branch2b          (BatchNorm)\n",
      "res4d_branch2c         (Conv2D)\n",
      "bn4d_branch2c          (BatchNorm)\n",
      "res4e_branch2a         (Conv2D)\n",
      "bn4e_branch2a          (BatchNorm)\n",
      "res4e_branch2b         (Conv2D)\n",
      "bn4e_branch2b          (BatchNorm)\n",
      "res4e_branch2c         (Conv2D)\n",
      "bn4e_branch2c          (BatchNorm)\n",
      "res4f_branch2a         (Conv2D)\n",
      "bn4f_branch2a          (BatchNorm)\n",
      "res4f_branch2b         (Conv2D)\n",
      "bn4f_branch2b          (BatchNorm)\n",
      "res4f_branch2c         (Conv2D)\n",
      "bn4f_branch2c          (BatchNorm)\n",
      "res5a_branch2a         (Conv2D)\n",
      "bn5a_branch2a          (BatchNorm)\n",
      "res5a_branch2b         (Conv2D)\n",
      "bn5a_branch2b          (BatchNorm)\n",
      "res5a_branch2c         (Conv2D)\n",
      "res5a_branch1          (Conv2D)\n",
      "bn5a_branch2c          (BatchNorm)\n",
      "bn5a_branch1           (BatchNorm)\n",
      "res5b_branch2a         (Conv2D)\n",
      "bn5b_branch2a          (BatchNorm)\n",
      "res5b_branch2b         (Conv2D)\n",
      "bn5b_branch2b          (BatchNorm)\n",
      "res5b_branch2c         (Conv2D)\n",
      "bn5b_branch2c          (BatchNorm)\n",
      "res5c_branch2a         (Conv2D)\n",
      "bn5c_branch2a          (BatchNorm)\n",
      "res5c_branch2b         (Conv2D)\n",
      "bn5c_branch2b          (BatchNorm)\n",
      "res5c_branch2c         (Conv2D)\n",
      "bn5c_branch2c          (BatchNorm)\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/13\n",
      " 310/1000 [========>.....................] - ETA: 33:36 - loss: 1.0741 - rpn_class_loss: 0.0168 - rpn_bbox_loss: 0.2528 - mrcnn_class_loss: 0.3381 - mrcnn_bbox_loss: 0.1970 - mrcnn_mask_loss: 0.2694"
     ]
    }
   ],
   "source": [
    "epo = 0\n",
    "LRs = [20, 15, 5, 10, 3, 5, 0.7, 0.3, 13, 0.7, 11, 17, 0.7, 0.3,20, 15, 5, 10]\n",
    "while epo < len(LRs):\n",
    "    train_set, valid_set = split_dateset(photo_meta_db, N_FOLDS, coco, label_names, cat_ids)\n",
    "\n",
    "    model.train(train_set, valid_set,\n",
    "            learning_rate = LR * LRs[epo],\n",
    "            epochs = epo + 8,\n",
    "            layers = 'all',\n",
    "            augmentation=None)\n",
    "    print(\"One epoch done, NO.: {}  LR: {}\".format(epo+8, LRs[epo]))\n",
    "    epo = epo + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
